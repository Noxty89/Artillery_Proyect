{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noxty89/Artillery_Proyect/blob/main/RNN_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMRxSRZf_6Uz"
      },
      "source": [
        "# Recurrent Neural Networks for classification\n",
        "**Adapted from Deep Learning with Pyton by Francois Chollet**\n",
        "\n",
        "https://github.com/fchollet/deep-learning-with-python-notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiwkQ6x5_6U1"
      },
      "source": [
        "## Recurrent Neural Networks\n",
        "\n",
        "`SimpleRNN` processes batches of sequences, like all other Keras layers. It takes inputs of shape `(batch_size, timesteps, input_features)`, rather than `(timesteps, input_features)`.\n",
        "\n",
        "Like all recurrent layers in Keras, `SimpleRNN` can be run in two different modes: it can return either the full sequences of successive outputs for each timestep (a 3D tensor of shape `(batch_size, timesteps, output_features)`), or it can return only the last output for each input sequence (a 2D tensor of shape `(batch_size, output_features)`). These two modes are controlled by the `return_sequences` constructor \n",
        "argument. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azrUdtxs_6U3"
      },
      "source": [
        "#### Embedding\n",
        "\n",
        "Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
        "\n",
        "This layer can only be used as the first layer in a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywi6SvoW_6U4"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKy-goDO_6VA"
      },
      "source": [
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vmko-VC2_6VG"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN, LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 16))\n",
        "model.add(SimpleRNN(32))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "826BDrDS_6VK"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HAQjZXV_6VO"
      },
      "source": [
        "To increase the representational power of a network several recurrent layers one after can be stacked with all intermediate layers returning full sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz11dzPs_6VP"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32)) # this last layer only returns the last outputs\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13DS_-6y_6VU"
      },
      "source": [
        "## IMDB movie review classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB9_UGei_6VV"
      },
      "source": [
        "Preprocess the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtX_NXhP_6VW"
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "max_features = 10000 # number of words to consider as features\n",
        "maxlen = 500 # cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32\n",
        "\n",
        "# problems with numpy latest version and load_data()\n",
        "# save np.load\n",
        "#np_load_old = np.load\n",
        "\n",
        "# modify the default parameters of np.load\n",
        "#np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "# call load_data with allow_pickle implicitly set to true\n",
        "\n",
        "print('Loading data...')\n",
        "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(input_train), 'train sequences')\n",
        "print(len(input_test), 'test sequences')\n",
        "\n",
        "# restore np.load for future normal usage\n",
        "#np.load = np_load_old\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "input_train = pad_sequences(input_train, maxlen=maxlen)\n",
        "input_test = pad_sequences(input_test, maxlen=maxlen)\n",
        "print('input_train shape:', input_train.shape)\n",
        "print('input_test shape:', input_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R0HWNHssCOSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rY9P80Is8lm"
      },
      "source": [
        "print(input_train[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "HNH6CUyZOP2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d997Fywc_6VZ"
      },
      "source": [
        "Train a simple recurrent network using an `Embedding` layer and a `SimpleRNN` layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiUBrkzn_6Va"
      },
      "source": [
        "from keras.layers import Dense, LSTM, GRU\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "#model.add(GRU(32))# LSTM, GRU\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjylRKgxATLO"
      },
      "source": [
        "history = model.fit(input_train, y_train,\n",
        "                   epochs=7,\n",
        "                   batch_size=128,\n",
        "                   validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eujHprrK_6Vd"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Resultado final:\")\n",
        "model.evaluate(input_test, y_test)"
      ],
      "metadata": {
        "id": "yjr-Pe2vQumJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises\n",
        "\n",
        "\n",
        "* Test with different recurrent units (SimpleRNN, LSTM, GRU).\n",
        "* Change the number of units in recurrent layer."
      ],
      "metadata": {
        "id": "BRwXchWRME5l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMXaqanD_6Vg"
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras.utils import pad_sequences\n",
        "from keras.layers import Dense, LSTM, GRU, SimpleRNN\n",
        "\n",
        "max_features = 10000 # number of words to consider as features\n",
        "maxlen = 500 # cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32\n",
        "\n",
        "# problems with numpy latest version and load_data()\n",
        "# save np.load\n",
        "#np_load_old = np.load\n",
        "\n",
        "# modify the default parameters of np.load\n",
        "#np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "# call load_data with allow_pickle implicitly set to true\n",
        "\n",
        "print('Loading data...')\n",
        "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(input_train), 'train sequences')\n",
        "print(len(input_test), 'test sequences')\n",
        "\n",
        "# restore np.load for future normal usage\n",
        "#np.load = np_load_old\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "input_train = pad_sequences(input_train, maxlen=maxlen)\n",
        "input_test = pad_sequences(input_test, maxlen=maxlen)\n",
        "print('input_train shape:', input_train.shape)\n",
        "print('input_test shape:', input_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "#model.add(SimpleRNN(32))\n",
        "#model.add(GRU(32))# LSTM, GRU\n",
        "model.add(LSTM(32))# LSTM, GRU\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(input_train, y_train,\n",
        "                   epochs=7,\n",
        "                   batch_size=128,\n",
        "                   validation_split=0.2)\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend();\n",
        "\n",
        "print(\"Resultado final:\")\n",
        "model.evaluate(input_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}